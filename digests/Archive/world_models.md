# World Models

Papers on world models for robotics, video prediction, and simulation.

**Last updated:** 2026-01-26 14:33 UTC

**Papers found:** 52

[Back to Home](../README.md)

---

## Papers with Project Pages / Code

### [CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning](https://arxiv.org/abs/2601.13304v1)

**Authors:** Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai et al. (9 authors)

**Published:** 2026-01-19 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.13304v1) | [PDF](https://arxiv.org/pdf/2601.13304v1.pdf) | [GitHub](https://github.com/CausalSpatial/CausalSpatial)

<details>
<summary>Abstract</summary>

Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer "what-if" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four task...

</details>

---

### [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538v1)

**Authors:** Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang et al. (29 authors)

**Published:** 2026-01-18 | **Categories:** cs.AI, cs.CL

**Links:** [arXiv](https://arxiv.org/abs/2601.12538v1) | [PDF](https://arxiv.org/pdf/2601.12538v1.pdf) | [GitHub](https://github.com/weitianxin/Awesome-Agentic-Reasoning)

<details>
<summary>Abstract</summary>

Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we charact...

</details>

---

### [MAD: Motion Appearance Decoupling for efficient Driving World Models](https://arxiv.org/abs/2601.09452v1)

**Authors:** Ahmad Rahimi, Valentin Gerard, Eloi Zablocki, Matthieu Cord, Alexandre Alahi

**Published:** 2026-01-14 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.09452v1) | [PDF](https://arxiv.org/pdf/2601.09452v1.pdf) | [Project Page](https://vita-epfl.github.io/MAD-World-Model/)

<details>
<summary>Abstract</summary>

Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving...

</details>

---

### [Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation](https://arxiv.org/abs/2601.07821v1)

**Authors:** Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang et al. (8 authors)

**Published:** 2026-01-12 | **Categories:** cs.RO, cs.AI, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.07821v1) | [PDF](https://arxiv.org/pdf/2601.07821v1.pdf) | [Project Page](https://failure-aware-rl.github.io)

<details>
<summary>Abstract</summary>

Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm ...

</details>

---

### [Can We Predict Before Executing Machine Learning Agents?](https://arxiv.org/abs/2601.05930v1)

**Authors:** Jingsheng Zheng, Jintian Zhang, Yujie Luo, Yuren Mao, Yunjun Gao et al. (8 authors)

**Published:** 2026-01-09 | **Categories:** cs.CL, cs.AI, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.05930v1) | [PDF](https://arxiv.org/pdf/2601.05930v1.pdf) | [GitHub](https://github.com/zjunlp/predict-before-execute)

<details>
<summary>Abstract</summary>

Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the ...

</details>

---

### [Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals](https://arxiv.org/abs/2601.05848v1)

**Authors:** Nate Gillman, Yinghua Zhou, Zitian Tang, Evan Luo, Arjan Chakravarthy et al. (9 authors)

**Published:** 2026-01-09 | **Categories:** cs.CV, cs.AI, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.05848v1) | [PDF](https://arxiv.org/pdf/2601.05848v1.pdf) | [Project Page](https://goal-force.github.io/)

<details>
<summary>Abstract</summary>

Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and i...

</details>

---

### [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138v1)

**Authors:** Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan et al. (6 authors)

**Published:** 2026-01-08 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.05138v1) | [PDF](https://arxiv.org/pdf/2601.05138v1.pdf) | [Project Page](https://sixiaozheng.github.io/VerseCrafter_page/)

<details>
<summary>Abstract</summary>

Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Contro...

</details>

---

### [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453v1)

**Authors:** Zhexiao Xiong, Xin Ye, Burhan Yaman, Sheng Cheng, Yiren Lu et al. (8 authors)

**Published:** 2026-01-07 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.04453v1) | [PDF](https://arxiv.org/pdf/2601.04453v1.pdf) | [Project Page](is)

<details>
<summary>Abstract</summary>

World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation withi...

</details>

---

### [PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation](https://arxiv.org/abs/2601.03782v1)

**Authors:** Wenlong Huang, Yu-Wei Chao, Arsalan Mousavian, Ming-Yu Liu, Dieter Fox et al. (7 authors)

**Published:** 2026-01-07 | **Categories:** cs.RO, cs.AI, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.03782v1) | [PDF](https://arxiv.org/pdf/2601.03782v1.pdf) | [Project Page](at)

<details>
<summary>Abstract</summary>

Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point...

</details>

---

### [Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions](https://arxiv.org/abs/2601.03590v1)

**Authors:** Zhongbin Guo, Zhen Yang, Yushan Li, Xinyue Zhang, Wenyu Gao et al. (9 authors)

**Published:** 2026-01-07 | **Categories:** cs.CV, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.03590v1) | [PDF](https://arxiv.org/pdf/2601.03590v1.pdf) | [GitHub](https://github.com/binisalegend/SiT-Bench)

<details>
<summary>Abstract</summary>

Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, r...

</details>

---

### [InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation](https://arxiv.org/abs/2601.02456v1)

**Authors:** Junhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He et al. (42 authors)

**Published:** 2026-01-05 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.02456v1) | [PDF](https://arxiv.org/pdf/2601.02456v1.pdf) | [Project Page](https://internrobotics.github.io/internvla-a1.github.io/)

<details>
<summary>Abstract</summary>

Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergi...

</details>

---

### [DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving](https://arxiv.org/abs/2601.01528v1)

**Authors:** Yang Zhou, Hao Shao, Letian Wang, Zhuofan Zong, Hongsheng Li et al. (6 authors)

**Published:** 2026-01-04 | **Categories:** cs.CV, cs.AI, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.01528v1) | [PDF](https://arxiv.org/pdf/2601.01528v1.pdf) | [Project Page](https://drivinggen-bench.github.io/)

<details>
<summary>Abstract</summary>

Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lac...

</details>

---

### [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393v1)

**Authors:** Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang et al. (6 authors)

**Published:** 2026-01-01 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.00393v1) | [PDF](https://arxiv.org/pdf/2601.00393v1.pdf) | [Project Page](is)

<details>
<summary>Abstract</summary>

In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos...

</details>

---

## Other Recent Papers

### [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163v1)

**Authors:** Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge et al. (11 authors)

**Published:** 2026-01-22 | **Categories:** cs.AI, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.16163v1) | [PDF](https://arxiv.org/pdf/2601.16163v1.pdf)

<details>
<summary>Abstract</summary>

Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effecti...

</details>

---

### [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007v1)

**Authors:** Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi et al. (19 authors)

**Published:** 2026-01-22 | **Categories:** cs.CV, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.16007v1) | [PDF](https://arxiv.org/pdf/2601.16007v1.pdf)

<details>
<summary>Abstract</summary>

Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce Physics...

</details>

---

### [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533v1)

**Authors:** Zhikang Chen, Tingting Zhu

**Published:** 2026-01-21 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.15533v1) | [PDF](https://arxiv.org/pdf/2601.15533v1.pdf)

<details>
<summary>Abstract</summary>

A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety...

</details>

---

### [Walk through Paintings: Egocentric World Models from Internet Priors](https://arxiv.org/abs/2601.15284v1)

**Authors:** Anurag Bagchi, Zhipeng Bao, Homanga Bharadhwaj, Yu-Xiong Wang, Pavel Tokmakov et al. (6 authors)

**Published:** 2026-01-21 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.15284v1) | [PDF](https://arxiv.org/pdf/2601.15284v1.pdf)

<details>
<summary>Abstract</summary>

What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video...

</details>

---

### [StableWorld: Towards Stable and Consistent Long Interactive Video Generation](https://arxiv.org/abs/2601.15281v1)

**Authors:** Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang et al. (9 authors)

**Published:** 2026-01-21 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.15281v1) | [PDF](https://arxiv.org/pdf/2601.15281v1.pdf)

<details>
<summary>Abstract</summary>

In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we ini...

</details>

---

### ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514v1)

**Authors:** Tony Chen, Sam Cheyette, Kelsey Allen, Joshua Tenenbaum, Kevin Smith

**Published:** 2026-01-20 | **Categories:** cs.AI, q-bio.NC

**Links:** [arXiv](https://arxiv.org/abs/2601.14514v1) | [PDF](https://arxiv.org/pdf/2601.14514v1.pdf)

<details>
<summary>Abstract</summary>

Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasonin...

</details>

---

### [VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models](https://arxiv.org/abs/2601.14354v1)

**Authors:** Yongchao Huang

**Published:** 2026-01-20 | **Categories:** cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.14354v1) | [PDF](https://arxiv.org/pdf/2601.14354v1.pdf)

<details>
<summary>Abstract</summary>

Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distributi...

</details>

---

### [Aligning Agentic World Models via Knowledgeable Experience Learning](https://arxiv.org/abs/2601.13247v1)

**Authors:** Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang et al. (6 authors)

**Published:** 2026-01-19 | **Categories:** cs.CL, cs.AI, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.13247v1) | [PDF](https://arxiv.org/pdf/2601.13247v1.pdf)

<details>
<summary>Abstract</summary>

Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which...

</details>

---

### [Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design](https://arxiv.org/abs/2601.12939v1)

**Authors:** Kaleem Arshid, Ali Krayani, Lucio Marcenaro, David Martin Gomez, Carlo Regazzoni

**Published:** 2026-01-19 | **Categories:** cs.RO, cs.AI, eess.SP

**Links:** [arXiv](https://arxiv.org/abs/2601.12939v1) | [PDF](https://arxiv.org/pdf/2601.12939v1.pdf)

<details>
<summary>Abstract</summary>

This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by mini...

</details>

---

### [ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models](https://arxiv.org/abs/2601.12428v1)

**Authors:** Baorui Peng, Wenyao Zhang, Liang Xu, Zekun Qi, Jiazhao Zhang et al. (8 authors)

**Published:** 2026-01-18 | **Categories:** cs.RO, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.12428v1) | [PDF](https://arxiv.org/pdf/2601.12428v1.pdf)

<details>
<summary>Abstract</summary>

Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world m...

</details>

---

### [An Efficient and Multi-Modal Navigation System with One-Step World Model](https://arxiv.org/abs/2601.12277v1)

**Authors:** Wangtian Shen, Ziyang Meng, Jinming Ma, Mingliang Zhou, Diyun Xiang

**Published:** 2026-01-18 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.12277v1) | [PDF](https://arxiv.org/pdf/2601.12277v1.pdf)

<details>
<summary>Abstract</summary>

Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imag...

</details>

---

### [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905v1)

**Authors:** Rajat Ghosh, Debojyoti Dutta

**Published:** 2026-01-15 | **Categories:** cs.LG, stat.ME

**Links:** [arXiv](https://arxiv.org/abs/2601.10905v1) | [PDF](https://arxiv.org/pdf/2601.10905v1.pdf)

<details>
<summary>Abstract</summary>

Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased sele...

</details>

---

### [Action100M: A Large-scale Video Action Dataset](https://arxiv.org/abs/2601.10592v1)

**Authors:** Delong Chen, Tejaswi Kasarla, Yejin Bang, Mustafa Shukor, Willy Chung et al. (9 authors)

**Published:** 2026-01-15 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.10592v1) | [PDF](https://arxiv.org/pdf/2601.10592v1.pdf)

<details>
<summary>Abstract</summary>

Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated ...

</details>

---

### [Inference-time Physics Alignment of Video Generative Models with Latent World Models](https://arxiv.org/abs/2601.10553v1)

**Authors:** Jianhao Yuan, Xiaofeng Zhang, Felix Friedrich, Nicolas Beltran-Velez, Melissa Hall et al. (10 authors)

**Published:** 2026-01-15 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.10553v1) | [PDF](https://arxiv.org/pdf/2601.10553v1.pdf)

<details>
<summary>Abstract</summary>

State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the ...

</details>

---

### [Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models](https://arxiv.org/abs/2601.08955v1)

**Authors:** Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li

**Published:** 2026-01-13 | **Categories:** cs.CL, cs.AI, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.08955v1) | [PDF](https://arxiv.org/pdf/2601.08955v1.pdf)

<details>
<summary>Abstract</summary>

Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yieldi...

</details>

---

### [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388v1)

**Authors:** Corina Chutaux

**Published:** 2026-01-13 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.08388v1) | [PDF](https://arxiv.org/pdf/2601.08388v1.pdf)

<details>
<summary>Abstract</summary>

Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrat...

</details>

---

### [Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling](https://arxiv.org/abs/2601.07964v1)

**Authors:** Alexander Boldachev

**Published:** 2026-01-12 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.07964v1) | [PDF](https://arxiv.org/pdf/2601.07964v1.pdf)

<details>
<summary>Abstract</summary>

This paper examines the application of Executable Ontologies (EO), implemented through the boldsea framework, to game development. We argue that EO represents a paradigm shift: a transition from algorithmic behavior programming to semantic world modeling, where agent behavior emerges naturally from declarative domain rules rather than being explicitly coded. Using a survival game scenario (Winter Feast), we demonstrate how EO achieves prioritybased task interruption through dataflow conditions r...

</details>

---

### [Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions](https://arxiv.org/abs/2601.07823v1)

**Authors:** Zhiting Mei, Tenny Yin, Ola Shorinwa, Apurva Badithela, Zhonghe Zheng et al. (12 authors)

**Published:** 2026-01-12 | **Categories:** eess.SY, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.07823v1) | [PDF](https://arxiv.org/pdf/2601.07823v1.pdf)

<details>
<summary>Abstract</summary>

Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-bo...

</details>

---

### [Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.07463v1)

**Authors:** Sijia li, Xinran Li, Shibo Chen, Jun Zhang

**Published:** 2026-01-12 | **Categories:** cs.AI, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.07463v1) | [PDF](https://arxiv.org/pdf/2601.07463v1.pdf)

<details>
<summary>Abstract</summary>

Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the ...

</details>

---

### [Object-Centric World Models Meet Monte Carlo Tree Search](https://arxiv.org/abs/2601.06604v1)

**Authors:** Rodion Vakhitov, Leonid Ugadiarov, Aleksandr Panov

**Published:** 2026-01-10 | **Categories:** cs.AI, cs.LG, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.06604v1) | [PDF](https://arxiv.org/pdf/2601.06604v1.pdf)

<details>
<summary>Abstract</summary>

In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's unde...

</details>

---

### [EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium](https://arxiv.org/abs/2601.05653v1)

**Authors:** Phu-Hoa Pham, Chi-Nguyen Tran, Duy-Minh Dao-Sy, Phu-Quy Nguyen-Lam, Trung-Kiet Huynh

**Published:** 2026-01-09 | **Categories:** cs.RO, cs.MA

**Links:** [arXiv](https://arxiv.org/abs/2601.05653v1) | [PDF](https://arxiv.org/pdf/2601.05653v1.pdf)

<details>
<summary>Abstract</summary>

Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quant...

</details>

---

### [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230v2)

**Authors:** Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun et al. (6 authors)

**Published:** 2026-01-08 | **Categories:** cs.AI, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.05230v2) | [PDF](https://arxiv.org/pdf/2601.05230v2.pdf)

<details>
<summary>Abstract</summary>

Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on ...

</details>

---

### [Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur](https://arxiv.org/abs/2601.06212v1)

**Authors:** Yani Meziani

**Published:** 2026-01-08 | **Categories:** cs.CV, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.06212v1) | [PDF](https://arxiv.org/pdf/2601.06212v1.pdf)

<details>
<summary>Abstract</summary>

We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Spla...

</details>

---

### [Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning](https://arxiv.org/abs/2601.04695v1)

**Authors:** Enze Pan

**Published:** 2026-01-08 | **Categories:** cs.AI, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.04695v1) | [PDF](https://arxiv.org/pdf/2601.04695v1.pdf)

<details>
<summary>Abstract</summary>

We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern eme...

</details>

---

### [Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead](https://arxiv.org/abs/2601.04686v1)

**Authors:** Oluwatosin Oseni, Shengjie Wang, Jun Zhu, Micah Corah

**Published:** 2026-01-08 | **Categories:** cs.LG, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.04686v1) | [PDF](https://arxiv.org/pdf/2601.04686v1.pdf)

<details>
<summary>Abstract</summary>

Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outp...

</details>

---

### [Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test](https://arxiv.org/abs/2601.04137v1)

**Authors:** Chun-Kai Fan, Xiaowei Chi, Xiaozhu Ju, Hao Li, Yong Bao et al. (21 authors)

**Published:** 2026-01-07 | **Categories:** cs.RO, cs.AI, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.04137v1) | [PDF](https://arxiv.org/pdf/2601.04137v1.pdf)

<details>
<summary>Abstract</summary>

As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to...

</details>

---

### [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035v1)

**Authors:** Yilin Cao, Yufeng Zhong, Zhixiong Zeng, Liming Zheng, Jing Huang et al. (9 authors)

**Published:** 2026-01-07 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.04035v1) | [PDF](https://arxiv.org/pdf/2601.04035v1.pdf)

<details>
<summary>Abstract</summary>

Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining e...

</details>

---

### [Current Agents Fail to Leverage World Model as Tool for Foresight](https://arxiv.org/abs/2601.03905v2)

**Authors:** Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang et al. (11 authors)

**Published:** 2026-01-07 | **Categories:** cs.AI, cs.CL, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.03905v2) | [PDF](https://arxiv.org/pdf/2601.03905v2.pdf)

<details>
<summary>Abstract</summary>

Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely in...

</details>

---

### [Semantic Belief-State World Model for 3D Human Motion Prediction](https://arxiv.org/abs/2601.03517v1)

**Authors:** Sarim Chaudhry

**Published:** 2026-01-07 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.03517v1) | [PDF](https://arxiv.org/pdf/2601.03517v1.pdf)

<details>
<summary>Abstract</summary>

Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond ...

</details>

---

### [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714v1)

**Authors:** Zhi Liu, Guangzhi Wang

**Published:** 2026-01-06 | **Categories:** cs.AI, cs.CL

**Links:** [arXiv](https://arxiv.org/abs/2601.02714v1) | [PDF](https://arxiv.org/pdf/2601.02714v1.pdf)

<details>
<summary>Abstract</summary>

Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities. Humans solve complex problems under limited cognitive resources through temporalized sequential reas...

</details>

---

### [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743v1)

**Authors:** Bin Xu

**Published:** 2026-01-05 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.01743v1) | [PDF](https://arxiv.org/pdf/2601.01743v1.pdf)

<details>
<summary>Abstract</summary>

AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical...

</details>

---

### [Explicit World Models for Reliable Human-Robot Collaboration](https://arxiv.org/abs/2601.01705v2)

**Authors:** Kenneth Kwok, Basura Fernando, Qianli Xu, Vigneshwaran Subbaraju, Dongkyu Choi et al. (6 authors)

**Published:** 2026-01-05 | **Categories:** cs.RO, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.01705v2) | [PDF](https://arxiv.org/pdf/2601.01705v2.pdf)

<details>
<summary>Abstract</summary>

This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consi...

</details>

---

### [HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller](https://arxiv.org/abs/2601.01577v1)

**Authors:** Tran Tien Dat, Nguyen Hai An, Nguyen Khanh Viet Dung, Nguyen Duy Duc

**Published:** 2026-01-04 | **Categories:** cs.RO, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.01577v1) | [PDF](https://arxiv.org/pdf/2601.01577v1.pdf)

<details>
<summary>Abstract</summary>

Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mi...

</details>

---

### [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321v1)

**Authors:** Rong Zhou, Dongping Chen, Zihan Jia, Yao Su, Yixin Liu et al. (27 authors)

**Published:** 2026-01-04 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.01321v1) | [PDF](https://arxiv.org/pdf/2601.01321v1.pdf)

<details>
<summary>Abstract</summary>

Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified ...

</details>

---

### [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075v1)

**Authors:** Hansen Jin Lillemark, Benhao Huang, Fangneng Zhan, Yilun Du, Thomas Anderson Keller

**Published:** 2026-01-03 | **Categories:** cs.LG, cs.AI, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.01075v1) | [PDF](https://arxiv.org/pdf/2601.01075v1.pdf)

<details>
<summary>Abstract</summary>

Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a fra...

</details>

---

### [AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation](https://arxiv.org/abs/2601.00930v1)

**Authors:** Nicolas Bougie, Gian Maria Marconi, Tony Yip, Narimasa Watanabe

**Published:** 2026-01-02 | **Categories:** cs.IR, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2601.00930v1) | [PDF](https://arxiv.org/pdf/2601.00930v1.pdf)

<details>
<summary>Abstract</summary>

Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. G...

</details>

---

### [Device-Native Autonomous Agents for Privacy-Preserving Negotiations](https://arxiv.org/abs/2601.00911v1)

**Authors:** Joyjit Roy

**Published:** 2026-01-01 | **Categories:** cs.CR, cs.AI, cs.ET

**Links:** [arXiv](https://arxiv.org/abs/2601.00911v1) | [PDF](https://arxiv.org/pdf/2601.00911v1.pdf)

<details>
<summary>Abstract</summary>

Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-tim...

</details>

---

### [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051v1)

**Authors:** Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng et al. (27 authors)

**Published:** 2025-12-31 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.00051v1) | [PDF](https://arxiv.org/pdf/2601.00051v1.pdf)

<details>
<summary>Abstract</summary>

World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framew...

</details>

---

### [LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving](https://arxiv.org/abs/2512.24712v2)

**Authors:** Qian Cheng, Weitao Zhou, Cheng Jing, Nanshan Deng, Junze Wen et al. (8 authors)

**Published:** 2025-12-31 | **Categories:** cs.RO, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2512.24712v2) | [PDF](https://arxiv.org/pdf/2512.24712v2.pdf)

<details>
<summary>Abstract</summary>

Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment. This work propos...

</details>

---
