# World Models

Papers on world models for robotics, video prediction, and simulation.

**Last updated:** 2026-02-07 16:30 UTC

**Papers found:** 3

[Back to Home](../README.md)

---

## Papers with Project Pages / Code

### [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)

**Authors:** András Balogh, Márk Jelasity

**Published:** 2026-02-05 | **Categories:** cs.LG, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2602.05903v1) | [PDF](https://arxiv.org/pdf/2602.05903v1.pdf) | [GitHub](https://github.com/szegedai/world-model-verification)

<details>
<summary>Abstract</summary>

Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify ...

</details>

---

### [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557v1)

**Authors:** Michael Schwingshackl, Fabio F. Oberweger, Mario Niedermeyer, Huemer Johannes, Markus Murschitz

**Published:** 2026-02-05 | **Categories:** cs.CV, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2602.05557v1) | [PDF](https://arxiv.org/pdf/2602.05557v1.pdf) | [GitHub](https://github.com/swingaxe/piratr)

<details>
<summary>Abstract</summary>

We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adj...

</details>

---

## Other Recent Papers

### [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001v1)

**Authors:** Carolina Higuera, Sergio Arnaud, Byron Boots, Mustafa Mukadam, Francois Robert Hogan et al. (6 authors)

**Published:** 2026-02-05 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2602.06001v1) | [PDF](https://arxiv.org/pdf/2602.06001v1.pdf)

<details>
<summary>Abstract</summary>

We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves ...

</details>

---
