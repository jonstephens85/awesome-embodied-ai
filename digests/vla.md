# Vision-Language-Action Models

Papers on VLAs and vision-language-action architectures for robotics.

**Last updated:** 2026-02-07 22:15 UTC

**Papers found:** 4

[Back to Home](../README.md)

---

## Papers with Project Pages / Code

### [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441v1)

**Authors:** Dean Fortier, Timothy Adamson, Tess Hellebrekers, Teresa LaScala, Kofi Ennin et al. (8 authors)

**Published:** 2026-02-05 | **Categories:** cs.RO, cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2602.05441v1) | [PDF](https://arxiv.org/pdf/2602.05441v1.pdf) | [Project Page](https://microsoft.github.io/BusyBox)

<details>
<summary>Abstract</summary>

Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to m...

</details>

---

## Other Recent Papers

### [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765v1)

**Authors:** Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai et al. (21 authors)

**Published:** 2026-02-05 | **Categories:** cs.AI

**Links:** [arXiv](https://arxiv.org/abs/2602.05765v1) | [PDF](https://arxiv.org/pdf/2602.05765v1.pdf)

<details>
<summary>Abstract</summary>

In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases ...

</details>

---

### [RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325v1)

**Authors:** Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang et al. (7 authors)

**Published:** 2026-02-05 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2602.05325v1) | [PDF](https://arxiv.org/pdf/2602.05325v1.pdf)

<details>
<summary>Abstract</summary>

Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, ...

</details>

---

### [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233v1)

**Authors:** Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang et al. (8 authors)

**Published:** 2026-02-05 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2602.05233v1) | [PDF](https://arxiv.org/pdf/2602.05233v1.pdf)

<details>
<summary>Abstract</summary>

Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulatio...

</details>

---
