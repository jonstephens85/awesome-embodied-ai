# Vision-Language-Action Models

Papers on VLAs and vision-language-action architectures for robotics.

**Last updated:** 2026-01-28 22:18 UTC

**Papers found:** 4

[Back to Home](../README.md)

---

## Papers with Project Pages / Code

### [Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods](https://arxiv.org/abs/2601.18723v1)

**Authors:** Mengyuan Liu, Juyi Sheng, Peiming Li, Ziyi Wang, Tianming Xu et al. (7 authors)

**Published:** 2026-01-26 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.18723v1) | [PDF](https://arxiv.org/pdf/2601.18723v1.pdf) | [Project Page](https://term-bench.github.io/)

<details>
<summary>Abstract</summary>

Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution ...

</details>

---

### [A Pragmatic VLA Foundation Model](https://arxiv.org/abs/2601.18692v1)

**Authors:** Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu et al. (25 authors)

**Published:** 2026-01-26 | **Categories:** cs.RO, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.18692v1) | [PDF](https://arxiv.org/pdf/2601.18692v1.pdf) | [Project Page](https://technology.robbyant.com/lingbot-vla/) | [GitHub](https://github.com/Robbyant/lingbot-vla/)

<details>
<summary>Abstract</summary>

Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per tas...

</details>

---

## Other Recent Papers

### [AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation](https://arxiv.org/abs/2601.19634v1)

**Authors:** Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu

**Published:** 2026-01-27 | **Categories:** cs.RO, cs.MM

**Links:** [arXiv](https://arxiv.org/abs/2601.19634v1) | [PDF](https://arxiv.org/pdf/2601.19634v1.pdf)

<details>
<summary>Abstract</summary>

Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we pro...

</details>

---

### [TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion](https://arxiv.org/abs/2601.18323v1)

**Authors:** Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin et al. (10 authors)

**Published:** 2026-01-26 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.18323v1) | [PDF](https://arxiv.org/pdf/2601.18323v1.pdf)

<details>
<summary>Abstract</summary>

The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajector...

</details>

---
