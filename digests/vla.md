# Vision-Language-Action Models

Papers on VLAs and vision-language-action architectures for robotics.

**Last updated:** 2026-01-29 16:44 UTC

**Papers found:** 4

[Back to Home](../README.md)

---

## Papers with Project Pages / Code

### [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334v1)

**Authors:** Brian Y. Tsui, Alan Y. Fang, Tiffany J. Hwu

**Published:** 2026-01-28 | **Categories:** cs.RO, cs.AI, cs.LG

**Links:** [arXiv](https://arxiv.org/abs/2601.20334v1) | [PDF](https://arxiv.org/pdf/2601.20334v1.pdf) | [GitHub](https://github.com/robiemusketeer/faea-sim)

<details>
<summary>Abstract</summary>

Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which appli...

</details>

---

## Other Recent Papers

### [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321v1)

**Authors:** Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li et al. (8 authors)

**Published:** 2026-01-28 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.20321v1) | [PDF](https://arxiv.org/pdf/2601.20321v1.pdf)

<details>
<summary>Abstract</summary>

Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation ...

</details>

---

### [Shallow-Ï€: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262v1)

**Authors:** Boseong Jeon, Yunho Choi, Taehan Kim

**Published:** 2026-01-28 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.20262v1) | [PDF](https://arxiv.org/pdf/2601.20262v1.pdf)

<details>
<summary>Abstract</summary>

The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled kn...

</details>

---

### [AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation](https://arxiv.org/abs/2601.19634v1)

**Authors:** Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu

**Published:** 2026-01-27 | **Categories:** cs.RO, cs.MM

**Links:** [arXiv](https://arxiv.org/abs/2601.19634v1) | [PDF](https://arxiv.org/pdf/2601.19634v1.pdf)

<details>
<summary>Abstract</summary>

Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we pro...

</details>

---
