# Vision-Language-Action Models

Papers on VLAs and vision-language-action architectures for robotics.

**Last updated:** 2026-01-27 23:49 UTC

**Papers found:** 5

[Back to Home](../README.md)

---

## Papers with Project Pages / Code

### [Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods](https://arxiv.org/abs/2601.18723v1)

**Authors:** Mengyuan Liu, Juyi Sheng, Peiming Li, Ziyi Wang, Tianming Xu et al. (7 authors)

**Published:** 2026-01-26 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.18723v1) | [PDF](https://arxiv.org/pdf/2601.18723v1.pdf) | [Project Page](https://term-bench.github.io/)

<details>
<summary>Abstract</summary>

Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution ...

</details>

---

### [A Pragmatic VLA Foundation Model](https://arxiv.org/abs/2601.18692v1)

**Authors:** Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu et al. (25 authors)

**Published:** 2026-01-26 | **Categories:** cs.RO, cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.18692v1) | [PDF](https://arxiv.org/pdf/2601.18692v1.pdf) | [Project Page](https://technology.robbyant.com/lingbot-vla/) | [GitHub](https://github.com/Robbyant/lingbot-vla/)

<details>
<summary>Abstract</summary>

Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per tas...

</details>

---

### [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885v1)

**Authors:** Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen et al. (10 authors)

**Published:** 2026-01-25 | **Categories:** cs.CV, cs.AI, cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.17885v1) | [PDF](https://arxiv.org/pdf/2601.17885v1.pdf) | [Project Page](https://peafowlvla.github.io/)

<details>
<summary>Abstract</summary>

Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding. In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA poli...

</details>

---

### [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657v1)

**Authors:** Taewan Cho, Taeryang Kim, Andrew Jaeyong Choi

**Published:** 2026-01-25 | **Categories:** cs.CV

**Links:** [arXiv](https://arxiv.org/abs/2601.17657v1) | [PDF](https://arxiv.org/pdf/2601.17657v1.pdf) | [GitHub](https://github.com/taewan2002/space-clip)

<details>
<summary>Abstract</summary>

Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a...

</details>

---

## Other Recent Papers

### [TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion](https://arxiv.org/abs/2601.18323v1)

**Authors:** Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin et al. (10 authors)

**Published:** 2026-01-26 | **Categories:** cs.RO

**Links:** [arXiv](https://arxiv.org/abs/2601.18323v1) | [PDF](https://arxiv.org/pdf/2601.18323v1.pdf)

<details>
<summary>Abstract</summary>

The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajector...

</details>

---
